import cv2
import mediapipe as mp
import pyttsx3
import threading
import queue

# Initialize MediaPipe Hands
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5)
mp_drawing = mp.solutions.drawing_utils

# Define a mapping of gestures to numbers (0-9)
gesture_to_number = {
    "0": 0,
    "1": 1,
    "2": 2,
    "3": 3,
    "4": 4,
    "5": 5,
    "6": 6,
    "7": 7,
    "8": 8,
    "9": 9,
    "thanks": "Thanks",
    "yes": "Yes",
    "no": "No",  # "No" gesture
    "iloveyou": "I Love You",
    "sorry": "Sorry",  # Added "sorry" gesture
}

# Create a queue for handling speech
speech_queue = queue.Queue()

# Initialize the pyttsx3 engine globally
engine = pyttsx3.init()

# Function to detect hands and draw landmarks
def detect_hands(image):
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = hands.process(image_rgb)
    recognized_gesture_number = None  # Initialize recognized_gesture_number variable

    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)
            recognized_gesture = recognize_gesture(hand_landmarks)
            if recognized_gesture:
                recognized_gesture_number = gesture_to_number.get(recognized_gesture)  # Get the corresponding gesture name
                # Convert recognized gesture to string before displaying
                cv2.putText(image, str(recognized_gesture_number), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                
    return image, recognized_gesture_number  # Return both image and recognized gesture number

# Function to recognize gestures based on landmarks
def recognize_gesture(hand_landmarks):
    # List of finger tip landmark indices for MediaPipe: index, middle, ring, pinky
    finger_tips = [8, 12, 16, 20]
    extended_fingers = [False] * 5  # Thumb, Index, Middle, Ring, Pinky
    
    # Check if fingers are extended by comparing tips to the second joint (further up the finger)
    for i, tip in enumerate(finger_tips):
        if hand_landmarks.landmark[tip].y < hand_landmarks.landmark[tip - 2].y:
            extended_fingers[i + 1] = True  # Skip thumb, set the rest to True if extended
    
    # Check if the thumb is extended
    thumb_tip = hand_landmarks.landmark[4].x
    thumb_base = hand_landmarks.landmark[2].x
    if thumb_tip < thumb_base:  # Assume thumb is extended to the left
        extended_fingers[0] = True

    # Define gestures based on extended finger patterns
    if extended_fingers == [False, False, False, False, False]:
        return "0"
    if extended_fingers == [False, True, False, False, False]:
        return "1"
    if extended_fingers == [False, True, True, False, False]:
        return "2"
    if extended_fingers == [False, True, True, True, False]:
        return "3"
    if extended_fingers == [False, True, True, True, True]:
        return "4"
    if extended_fingers == [True, True, True, True, True]:
        return "5"

    # Check specific thumb positions for numbers 6-9
    thumb_tip_x = hand_landmarks.landmark[4].x
    thumb_tip_y = hand_landmarks.landmark[4].y
    
    if abs(thumb_tip_x - hand_landmarks.landmark[8].x) < 0.05 and abs(thumb_tip_y - hand_landmarks.landmark[8].y) < 0.05:
        return "6"
    if abs(thumb_tip_x - hand_landmarks.landmark[12].x) < 0.05 and abs(thumb_tip_y - hand_landmarks.landmark[12].y) < 0.05:
        return "7"
    if abs(thumb_tip_x - hand_landmarks.landmark[16].x) < 0.05 and abs(thumb_tip_y - hand_landmarks.landmark[16].y) < 0.05:
        return "8"
    if extended_fingers == [True, True, True, True, False]:
        return "9"

    # Update the "thanks" gesture based on the new finger configuration:
    # Thumb, index, and middle fingers open, ring and pinky closed.
    if extended_fingers == [True, True, True, False, False]:
        return "thanks"
    
    # Add the "sorry" gesture: Thumb and pinky open, remaining fingers closed
    if extended_fingers == [True, False, False, False, True]:
        return "sorry"

    # Update the "no" gesture: Thumb and index fingers extended, remaining closed
    if extended_fingers == [True, True, False, False, False]:
        return "no"

    # Detect additional gestures based on hand configurations
    if extended_fingers == [True, False, False, False, False]:
        return "yes"
    if abs(thumb_tip_x - hand_landmarks.landmark[8].x) < 0.05 and abs(thumb_tip_y - hand_landmarks.landmark[8].y) < 0.05:
        if abs(thumb_tip_x - hand_landmarks.landmark[12].x) < 0.05 and abs(thumb_tip_y - hand_landmarks.landmark[12].y) < 0.05:
            if abs(thumb_tip_x - hand_landmarks.landmark[16].x) < 0.05 and abs(thumb_tip_y - hand_landmarks.landmark[16].y) < 0.05:
                if abs(thumb_tip_x - hand_landmarks.landmark[20].x) < 0.05 and abs(thumb_tip_y - hand_landmarks.landmark[20].y) < 0.05:
                    return "no"
    if extended_fingers == [True, True, False, False, True]:
        return "iloveyou"
    
    return None  # If no valid gesture is recognized

# Function to speak the gesture using pyttsx3
def speak_gesture(gesture_name):
    engine.say(gesture_name)
    engine.runAndWait()

# Threading function to safely handle speech in the background
def handle_gesture_speech(gesture_name):
    speech_queue.put(gesture_name)

def process_speech_queue():
    while True:
        gesture_name = speech_queue.get()
        if gesture_name is None:
            break
        speak_gesture(gesture_name)

previous_gesture_number = None  # Store previous gesture to prevent speaking every frame

# Start capturing video from webcam
cap = cv2.VideoCapture(0)

# Start the speech processing in a separate thread
speech_thread = threading.Thread(target=process_speech_queue, daemon=True)
speech_thread.start()

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    frame = cv2.flip(frame, 1)  # Flip the frame horizontally
    frame, recognized_gesture_number = detect_hands(frame)  # Capture both frame and recognized gesture number

    # Ensure the recognized gesture is always a string for speaking
    if isinstance(recognized_gesture_number, int):
        recognized_gesture_number = str(recognized_gesture_number)

    # Only speak when the gesture has changed
    if recognized_gesture_number != previous_gesture_number:
        if recognized_gesture_number is not None:
            print(f"Recognized Gesture: {recognized_gesture_number}")
            handle_gesture_speech(recognized_gesture_number)  # Add gesture to speech queue
        previous_gesture_number = recognized_gesture_number

    cv2.imshow('Sign Language Detection', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

